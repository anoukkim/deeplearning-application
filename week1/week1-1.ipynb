{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝응용 1주차 리포트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 당뇨병 데이터를 가지고 머신러닝 5가지 분류를 수행.\n",
    "#### (SVM, LR, RF, DT, KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Target\n",
    "x = df.drop(['Outcome'], axis=1)\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85       102\n",
      "           1       0.77      0.52      0.62        52\n",
      "\n",
      "    accuracy                           0.79       154\n",
      "   macro avg       0.78      0.72      0.74       154\n",
      "weighted avg       0.78      0.79      0.77       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Train\n",
    "from sklearn import svm\n",
    "model = svm.SVC()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# SVM Test\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"SVM Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83       102\n",
      "           1       0.71      0.52      0.60        52\n",
      "\n",
      "    accuracy                           0.77       154\n",
      "   macro avg       0.75      0.71      0.72       154\n",
      "weighted avg       0.76      0.77      0.76       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LR Train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# LR Test\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"LR Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85       102\n",
      "           1       0.73      0.58      0.65        52\n",
      "\n",
      "    accuracy                           0.79       154\n",
      "   macro avg       0.77      0.73      0.75       154\n",
      "weighted avg       0.78      0.79      0.78       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RF Train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# RF Test\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"RF Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75       102\n",
      "           1       0.52      0.56      0.54        52\n",
      "\n",
      "    accuracy                           0.68       154\n",
      "   macro avg       0.64      0.65      0.64       154\n",
      "weighted avg       0.68      0.68      0.68       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DT Train\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# DT Test\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"DT Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82       102\n",
      "           1       0.65      0.60      0.62        52\n",
      "\n",
      "    accuracy                           0.75       154\n",
      "   macro avg       0.72      0.71      0.72       154\n",
      "weighted avg       0.75      0.75      0.75       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN Train\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# KNN Test\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"KNN Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 동일한 데이터로 딥러닝 분류 수행하라. (dense layer 만 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "x = df.drop(['Outcome'], axis=1).values\n",
    "y = df['Outcome'].values\n",
    "\n",
    "# Train-Test Split + Data Normalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "scaler = StandardScaler()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] ---------------------------\n",
      "Test Accuracy: 75.3%, Avg loss: 0.562564 \n",
      "\n",
      "Epoch [2/20] ---------------------------\n",
      "Test Accuracy: 76.6%, Avg loss: 0.525923 \n",
      "\n",
      "Epoch [3/20] ---------------------------\n",
      "Test Accuracy: 77.3%, Avg loss: 0.510422 \n",
      "\n",
      "Epoch [4/20] ---------------------------\n",
      "Test Accuracy: 77.3%, Avg loss: 0.505820 \n",
      "\n",
      "Epoch [5/20] ---------------------------\n",
      "Test Accuracy: 78.6%, Avg loss: 0.502035 \n",
      "\n",
      "Epoch [6/20] ---------------------------\n",
      "Test Accuracy: 76.6%, Avg loss: 0.507794 \n",
      "\n",
      "Epoch [7/20] ---------------------------\n",
      "Test Accuracy: 76.6%, Avg loss: 0.506794 \n",
      "\n",
      "Epoch [8/20] ---------------------------\n",
      "Test Accuracy: 76.0%, Avg loss: 0.505237 \n",
      "\n",
      "Epoch [9/20] ---------------------------\n",
      "Test Accuracy: 78.6%, Avg loss: 0.505346 \n",
      "\n",
      "Epoch [10/20] ---------------------------\n",
      "Test Accuracy: 77.3%, Avg loss: 0.509401 \n",
      "\n",
      "Epoch [11/20] ---------------------------\n",
      "Test Accuracy: 76.6%, Avg loss: 0.509235 \n",
      "\n",
      "Epoch [12/20] ---------------------------\n",
      "Test Accuracy: 77.3%, Avg loss: 0.511531 \n",
      "\n",
      "Epoch [13/20] ---------------------------\n",
      "Test Accuracy: 77.3%, Avg loss: 0.511222 \n",
      "\n",
      "Epoch [14/20] ---------------------------\n",
      "Test Accuracy: 77.3%, Avg loss: 0.514290 \n",
      "\n",
      "Epoch [15/20] ---------------------------\n",
      "Test Accuracy: 77.3%, Avg loss: 0.513928 \n",
      "\n",
      "Epoch [16/20] ---------------------------\n",
      "Test Accuracy: 76.6%, Avg loss: 0.517244 \n",
      "\n",
      "Epoch [17/20] ---------------------------\n",
      "Test Accuracy: 76.6%, Avg loss: 0.515555 \n",
      "\n",
      "Epoch [18/20] ---------------------------\n",
      "Test Accuracy: 76.6%, Avg loss: 0.522060 \n",
      "\n",
      "Epoch [19/20] ---------------------------\n",
      "Test Accuracy: 76.6%, Avg loss: 0.518279 \n",
      "\n",
      "Epoch [20/20] ---------------------------\n",
      "Test Accuracy: 76.0%, Avg loss: 0.522727 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# NeuralNetwork Class (Only Dense Layers)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(n_input, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "n_input = x_train.shape[1]   \n",
    "n_output = 2                 \n",
    "n_hidden = 64                \n",
    "\n",
    "# Functions\n",
    "model = NeuralNetwork(n_input, n_output, n_hidden).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train Function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train() \n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Test Function\n",
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:.6f} \\n\")\n",
    "\n",
    "# Train + Test Loop\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch [{t+1}/{epochs}] ---------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 해당 데이터에서 Outcome을 삭제하고 BMI를 예측하는 회귀를 수행하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Outcome' + Target → BMI\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "x = df.drop(['Outcome', 'BMI'], axis=1)\n",
    "y = df['BMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 39.880241751199044\n"
     ]
    }
   ],
   "source": [
    "# LR Train\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# LR Test\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 3번과 동일하지만 dense layer만 사용한 신경만으로 회귀를 수행하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "x = df.drop(['BMI'], axis=1).values  \n",
    "y = df['BMI'].values  \n",
    "\n",
    "# Train-Test Split + Data Normalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "scaler = StandardScaler()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Make it 2D for regression\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Dataset and Dataloader\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200]\n",
      "Avg loss: 1068.450671 \n",
      "\n",
      "------------------------------\n",
      "Epoch [2/200]\n",
      "Avg loss: 1040.725122 \n",
      "\n",
      "------------------------------\n",
      "Epoch [3/200]\n",
      "Avg loss: 1010.751501 \n",
      "\n",
      "------------------------------\n",
      "Epoch [4/200]\n",
      "Avg loss: 977.090796 \n",
      "\n",
      "------------------------------\n",
      "Epoch [5/200]\n",
      "Avg loss: 938.284888 \n",
      "\n",
      "------------------------------\n",
      "Epoch [6/200]\n",
      "Avg loss: 894.323669 \n",
      "\n",
      "------------------------------\n",
      "Epoch [7/200]\n",
      "Avg loss: 845.365283 \n",
      "\n",
      "------------------------------\n",
      "Epoch [8/200]\n",
      "Avg loss: 790.613672 \n",
      "\n",
      "------------------------------\n",
      "Epoch [9/200]\n",
      "Avg loss: 732.682898 \n",
      "\n",
      "------------------------------\n",
      "Epoch [10/200]\n",
      "Avg loss: 669.999011 \n",
      "\n",
      "------------------------------\n",
      "Epoch [11/200]\n",
      "Avg loss: 604.911230 \n",
      "\n",
      "------------------------------\n",
      "Epoch [12/200]\n",
      "Avg loss: 539.889075 \n",
      "\n",
      "------------------------------\n",
      "Epoch [13/200]\n",
      "Avg loss: 475.071393 \n",
      "\n",
      "------------------------------\n",
      "Epoch [14/200]\n",
      "Avg loss: 413.425745 \n",
      "\n",
      "------------------------------\n",
      "Epoch [15/200]\n",
      "Avg loss: 356.209662 \n",
      "\n",
      "------------------------------\n",
      "Epoch [16/200]\n",
      "Avg loss: 303.004083 \n",
      "\n",
      "------------------------------\n",
      "Epoch [17/200]\n",
      "Avg loss: 256.399692 \n",
      "\n",
      "------------------------------\n",
      "Epoch [18/200]\n",
      "Avg loss: 216.409027 \n",
      "\n",
      "------------------------------\n",
      "Epoch [19/200]\n",
      "Avg loss: 182.895435 \n",
      "\n",
      "------------------------------\n",
      "Epoch [20/200]\n",
      "Avg loss: 156.780096 \n",
      "\n",
      "------------------------------\n",
      "Epoch [21/200]\n",
      "Avg loss: 135.491666 \n",
      "\n",
      "------------------------------\n",
      "Epoch [22/200]\n",
      "Avg loss: 119.671964 \n",
      "\n",
      "------------------------------\n",
      "Epoch [23/200]\n",
      "Avg loss: 107.959596 \n",
      "\n",
      "------------------------------\n",
      "Epoch [24/200]\n",
      "Avg loss: 99.758298 \n",
      "\n",
      "------------------------------\n",
      "Epoch [25/200]\n",
      "Avg loss: 93.846078 \n",
      "\n",
      "------------------------------\n",
      "Epoch [26/200]\n",
      "Avg loss: 90.014302 \n",
      "\n",
      "------------------------------\n",
      "Epoch [27/200]\n",
      "Avg loss: 86.713892 \n",
      "\n",
      "------------------------------\n",
      "Epoch [28/200]\n",
      "Avg loss: 84.126309 \n",
      "\n",
      "------------------------------\n",
      "Epoch [29/200]\n",
      "Avg loss: 82.651472 \n",
      "\n",
      "------------------------------\n",
      "Epoch [30/200]\n",
      "Avg loss: 81.296265 \n",
      "\n",
      "------------------------------\n",
      "Epoch [31/200]\n",
      "Avg loss: 80.373541 \n",
      "\n",
      "------------------------------\n",
      "Epoch [32/200]\n",
      "Avg loss: 79.392976 \n",
      "\n",
      "------------------------------\n",
      "Epoch [33/200]\n",
      "Avg loss: 78.531098 \n",
      "\n",
      "------------------------------\n",
      "Epoch [34/200]\n",
      "Avg loss: 77.994756 \n",
      "\n",
      "------------------------------\n",
      "Epoch [35/200]\n",
      "Avg loss: 77.146444 \n",
      "\n",
      "------------------------------\n",
      "Epoch [36/200]\n",
      "Avg loss: 76.415681 \n",
      "\n",
      "------------------------------\n",
      "Epoch [37/200]\n",
      "Avg loss: 75.627081 \n",
      "\n",
      "------------------------------\n",
      "Epoch [38/200]\n",
      "Avg loss: 75.222699 \n",
      "\n",
      "------------------------------\n",
      "Epoch [39/200]\n",
      "Avg loss: 74.886979 \n",
      "\n",
      "------------------------------\n",
      "Epoch [40/200]\n",
      "Avg loss: 74.066679 \n",
      "\n",
      "------------------------------\n",
      "Epoch [41/200]\n",
      "Avg loss: 73.834253 \n",
      "\n",
      "------------------------------\n",
      "Epoch [42/200]\n",
      "Avg loss: 73.155112 \n",
      "\n",
      "------------------------------\n",
      "Epoch [43/200]\n",
      "Avg loss: 72.828555 \n",
      "\n",
      "------------------------------\n",
      "Epoch [44/200]\n",
      "Avg loss: 72.552666 \n",
      "\n",
      "------------------------------\n",
      "Epoch [45/200]\n",
      "Avg loss: 72.136531 \n",
      "\n",
      "------------------------------\n",
      "Epoch [46/200]\n",
      "Avg loss: 71.530965 \n",
      "\n",
      "------------------------------\n",
      "Epoch [47/200]\n",
      "Avg loss: 71.645065 \n",
      "\n",
      "------------------------------\n",
      "Epoch [48/200]\n",
      "Avg loss: 71.036962 \n",
      "\n",
      "------------------------------\n",
      "Epoch [49/200]\n",
      "Avg loss: 71.081367 \n",
      "\n",
      "------------------------------\n",
      "Epoch [50/200]\n",
      "Avg loss: 70.330479 \n",
      "\n",
      "------------------------------\n",
      "Epoch [51/200]\n",
      "Avg loss: 70.036349 \n",
      "\n",
      "------------------------------\n",
      "Epoch [52/200]\n",
      "Avg loss: 69.659968 \n",
      "\n",
      "------------------------------\n",
      "Epoch [53/200]\n",
      "Avg loss: 69.306609 \n",
      "\n",
      "------------------------------\n",
      "Epoch [54/200]\n",
      "Avg loss: 68.901744 \n",
      "\n",
      "------------------------------\n",
      "Epoch [55/200]\n",
      "Avg loss: 68.669366 \n",
      "\n",
      "------------------------------\n",
      "Epoch [56/200]\n",
      "Avg loss: 68.556046 \n",
      "\n",
      "------------------------------\n",
      "Epoch [57/200]\n",
      "Avg loss: 68.252586 \n",
      "\n",
      "------------------------------\n",
      "Epoch [58/200]\n",
      "Avg loss: 67.874674 \n",
      "\n",
      "------------------------------\n",
      "Epoch [59/200]\n",
      "Avg loss: 67.480360 \n",
      "\n",
      "------------------------------\n",
      "Epoch [60/200]\n",
      "Avg loss: 67.286790 \n",
      "\n",
      "------------------------------\n",
      "Epoch [61/200]\n",
      "Avg loss: 67.334199 \n",
      "\n",
      "------------------------------\n",
      "Epoch [62/200]\n",
      "Avg loss: 67.030993 \n",
      "\n",
      "------------------------------\n",
      "Epoch [63/200]\n",
      "Avg loss: 66.588096 \n",
      "\n",
      "------------------------------\n",
      "Epoch [64/200]\n",
      "Avg loss: 66.516085 \n",
      "\n",
      "------------------------------\n",
      "Epoch [65/200]\n",
      "Avg loss: 66.409715 \n",
      "\n",
      "------------------------------\n",
      "Epoch [66/200]\n",
      "Avg loss: 66.134797 \n",
      "\n",
      "------------------------------\n",
      "Epoch [67/200]\n",
      "Avg loss: 65.772344 \n",
      "\n",
      "------------------------------\n",
      "Epoch [68/200]\n",
      "Avg loss: 65.751413 \n",
      "\n",
      "------------------------------\n",
      "Epoch [69/200]\n",
      "Avg loss: 65.607325 \n",
      "\n",
      "------------------------------\n",
      "Epoch [70/200]\n",
      "Avg loss: 65.441530 \n",
      "\n",
      "------------------------------\n",
      "Epoch [71/200]\n",
      "Avg loss: 65.302935 \n",
      "\n",
      "------------------------------\n",
      "Epoch [72/200]\n",
      "Avg loss: 64.785795 \n",
      "\n",
      "------------------------------\n",
      "Epoch [73/200]\n",
      "Avg loss: 64.701399 \n",
      "\n",
      "------------------------------\n",
      "Epoch [74/200]\n",
      "Avg loss: 64.682722 \n",
      "\n",
      "------------------------------\n",
      "Epoch [75/200]\n",
      "Avg loss: 64.405083 \n",
      "\n",
      "------------------------------\n",
      "Epoch [76/200]\n",
      "Avg loss: 64.250630 \n",
      "\n",
      "------------------------------\n",
      "Epoch [77/200]\n",
      "Avg loss: 64.050978 \n",
      "\n",
      "------------------------------\n",
      "Epoch [78/200]\n",
      "Avg loss: 63.952274 \n",
      "\n",
      "------------------------------\n",
      "Epoch [79/200]\n",
      "Avg loss: 63.842427 \n",
      "\n",
      "------------------------------\n",
      "Epoch [80/200]\n",
      "Avg loss: 63.593109 \n",
      "\n",
      "------------------------------\n",
      "Epoch [81/200]\n",
      "Avg loss: 63.531394 \n",
      "\n",
      "------------------------------\n",
      "Epoch [82/200]\n",
      "Avg loss: 63.633829 \n",
      "\n",
      "------------------------------\n",
      "Epoch [83/200]\n",
      "Avg loss: 63.418669 \n",
      "\n",
      "------------------------------\n",
      "Epoch [84/200]\n",
      "Avg loss: 63.166149 \n",
      "\n",
      "------------------------------\n",
      "Epoch [85/200]\n",
      "Avg loss: 62.955418 \n",
      "\n",
      "------------------------------\n",
      "Epoch [86/200]\n",
      "Avg loss: 62.354058 \n",
      "\n",
      "------------------------------\n",
      "Epoch [87/200]\n",
      "Avg loss: 62.497359 \n",
      "\n",
      "------------------------------\n",
      "Epoch [88/200]\n",
      "Avg loss: 62.189653 \n",
      "\n",
      "------------------------------\n",
      "Epoch [89/200]\n",
      "Avg loss: 62.219662 \n",
      "\n",
      "------------------------------\n",
      "Epoch [90/200]\n",
      "Avg loss: 62.147311 \n",
      "\n",
      "------------------------------\n",
      "Epoch [91/200]\n",
      "Avg loss: 62.131133 \n",
      "\n",
      "------------------------------\n",
      "Epoch [92/200]\n",
      "Avg loss: 61.646983 \n",
      "\n",
      "------------------------------\n",
      "Epoch [93/200]\n",
      "Avg loss: 61.739956 \n",
      "\n",
      "------------------------------\n",
      "Epoch [94/200]\n",
      "Avg loss: 61.845374 \n",
      "\n",
      "------------------------------\n",
      "Epoch [95/200]\n",
      "Avg loss: 61.562468 \n",
      "\n",
      "------------------------------\n",
      "Epoch [96/200]\n",
      "Avg loss: 61.533248 \n",
      "\n",
      "------------------------------\n",
      "Epoch [97/200]\n",
      "Avg loss: 61.662881 \n",
      "\n",
      "------------------------------\n",
      "Epoch [98/200]\n",
      "Avg loss: 61.428356 \n",
      "\n",
      "------------------------------\n",
      "Epoch [99/200]\n",
      "Avg loss: 61.372874 \n",
      "\n",
      "------------------------------\n",
      "Epoch [100/200]\n",
      "Avg loss: 61.481297 \n",
      "\n",
      "------------------------------\n",
      "Epoch [101/200]\n",
      "Avg loss: 61.390820 \n",
      "\n",
      "------------------------------\n",
      "Epoch [102/200]\n",
      "Avg loss: 61.245301 \n",
      "\n",
      "------------------------------\n",
      "Epoch [103/200]\n",
      "Avg loss: 61.181616 \n",
      "\n",
      "------------------------------\n",
      "Epoch [104/200]\n",
      "Avg loss: 60.828596 \n",
      "\n",
      "------------------------------\n",
      "Epoch [105/200]\n",
      "Avg loss: 60.710464 \n",
      "\n",
      "------------------------------\n",
      "Epoch [106/200]\n",
      "Avg loss: 60.587303 \n",
      "\n",
      "------------------------------\n",
      "Epoch [107/200]\n",
      "Avg loss: 60.764753 \n",
      "\n",
      "------------------------------\n",
      "Epoch [108/200]\n",
      "Avg loss: 60.659476 \n",
      "\n",
      "------------------------------\n",
      "Epoch [109/200]\n",
      "Avg loss: 60.911534 \n",
      "\n",
      "------------------------------\n",
      "Epoch [110/200]\n",
      "Avg loss: 60.721073 \n",
      "\n",
      "------------------------------\n",
      "Epoch [111/200]\n",
      "Avg loss: 60.686777 \n",
      "\n",
      "------------------------------\n",
      "Epoch [112/200]\n",
      "Avg loss: 60.428144 \n",
      "\n",
      "------------------------------\n",
      "Epoch [113/200]\n",
      "Avg loss: 60.671066 \n",
      "\n",
      "------------------------------\n",
      "Epoch [114/200]\n",
      "Avg loss: 60.646575 \n",
      "\n",
      "------------------------------\n",
      "Epoch [115/200]\n",
      "Avg loss: 60.850116 \n",
      "\n",
      "------------------------------\n",
      "Epoch [116/200]\n",
      "Avg loss: 60.626562 \n",
      "\n",
      "------------------------------\n",
      "Epoch [117/200]\n",
      "Avg loss: 60.608030 \n",
      "\n",
      "------------------------------\n",
      "Epoch [118/200]\n",
      "Avg loss: 60.716833 \n",
      "\n",
      "------------------------------\n",
      "Epoch [119/200]\n",
      "Avg loss: 60.464329 \n",
      "\n",
      "------------------------------\n",
      "Epoch [120/200]\n",
      "Avg loss: 60.620837 \n",
      "\n",
      "------------------------------\n",
      "Epoch [121/200]\n",
      "Avg loss: 60.489444 \n",
      "\n",
      "------------------------------\n",
      "Epoch [122/200]\n",
      "Avg loss: 60.378436 \n",
      "\n",
      "------------------------------\n",
      "Epoch [123/200]\n",
      "Avg loss: 60.039627 \n",
      "\n",
      "------------------------------\n",
      "Epoch [124/200]\n",
      "Avg loss: 60.110526 \n",
      "\n",
      "------------------------------\n",
      "Epoch [125/200]\n",
      "Avg loss: 60.054791 \n",
      "\n",
      "------------------------------\n",
      "Epoch [126/200]\n",
      "Avg loss: 60.025019 \n",
      "\n",
      "------------------------------\n",
      "Epoch [127/200]\n",
      "Avg loss: 59.662640 \n",
      "\n",
      "------------------------------\n",
      "Epoch [128/200]\n",
      "Avg loss: 59.498700 \n",
      "\n",
      "------------------------------\n",
      "Epoch [129/200]\n",
      "Avg loss: 59.480613 \n",
      "\n",
      "------------------------------\n",
      "Epoch [130/200]\n",
      "Avg loss: 59.998093 \n",
      "\n",
      "------------------------------\n",
      "Epoch [131/200]\n",
      "Avg loss: 59.803756 \n",
      "\n",
      "------------------------------\n",
      "Epoch [132/200]\n",
      "Avg loss: 59.732171 \n",
      "\n",
      "------------------------------\n",
      "Epoch [133/200]\n",
      "Avg loss: 59.770239 \n",
      "\n",
      "------------------------------\n",
      "Epoch [134/200]\n",
      "Avg loss: 59.772122 \n",
      "\n",
      "------------------------------\n",
      "Epoch [135/200]\n",
      "Avg loss: 59.971498 \n",
      "\n",
      "------------------------------\n",
      "Epoch [136/200]\n",
      "Avg loss: 59.757433 \n",
      "\n",
      "------------------------------\n",
      "Epoch [137/200]\n",
      "Avg loss: 59.762104 \n",
      "\n",
      "------------------------------\n",
      "Epoch [138/200]\n",
      "Avg loss: 59.613909 \n",
      "\n",
      "------------------------------\n",
      "Epoch [139/200]\n",
      "Avg loss: 59.633841 \n",
      "\n",
      "------------------------------\n",
      "Epoch [140/200]\n",
      "Avg loss: 60.052776 \n",
      "\n",
      "------------------------------\n",
      "Epoch [141/200]\n",
      "Avg loss: 59.936406 \n",
      "\n",
      "------------------------------\n",
      "Epoch [142/200]\n",
      "Avg loss: 59.883299 \n",
      "\n",
      "------------------------------\n",
      "Epoch [143/200]\n",
      "Avg loss: 59.673605 \n",
      "\n",
      "------------------------------\n",
      "Epoch [144/200]\n",
      "Avg loss: 59.719399 \n",
      "\n",
      "------------------------------\n",
      "Epoch [145/200]\n",
      "Avg loss: 59.972407 \n",
      "\n",
      "------------------------------\n",
      "Epoch [146/200]\n",
      "Avg loss: 59.777563 \n",
      "\n",
      "------------------------------\n",
      "Epoch [147/200]\n",
      "Avg loss: 59.670954 \n",
      "\n",
      "------------------------------\n",
      "Epoch [148/200]\n",
      "Avg loss: 59.785109 \n",
      "\n",
      "------------------------------\n",
      "Epoch [149/200]\n",
      "Avg loss: 59.893410 \n",
      "\n",
      "------------------------------\n",
      "Epoch [150/200]\n",
      "Avg loss: 59.820765 \n",
      "\n",
      "------------------------------\n",
      "Epoch [151/200]\n",
      "Avg loss: 59.995022 \n",
      "\n",
      "------------------------------\n",
      "Epoch [152/200]\n",
      "Avg loss: 59.514005 \n",
      "\n",
      "------------------------------\n",
      "Epoch [153/200]\n",
      "Avg loss: 59.650268 \n",
      "\n",
      "------------------------------\n",
      "Epoch [154/200]\n",
      "Avg loss: 59.808771 \n",
      "\n",
      "------------------------------\n",
      "Epoch [155/200]\n",
      "Avg loss: 59.861272 \n",
      "\n",
      "------------------------------\n",
      "Epoch [156/200]\n",
      "Avg loss: 59.788116 \n",
      "\n",
      "------------------------------\n",
      "Epoch [157/200]\n",
      "Avg loss: 59.811335 \n",
      "\n",
      "------------------------------\n",
      "Epoch [158/200]\n",
      "Avg loss: 59.562155 \n",
      "\n",
      "------------------------------\n",
      "Epoch [159/200]\n",
      "Avg loss: 59.812860 \n",
      "\n",
      "------------------------------\n",
      "Epoch [160/200]\n",
      "Avg loss: 59.658321 \n",
      "\n",
      "------------------------------\n",
      "Epoch [161/200]\n",
      "Avg loss: 59.852917 \n",
      "\n",
      "------------------------------\n",
      "Epoch [162/200]\n",
      "Avg loss: 59.717606 \n",
      "\n",
      "------------------------------\n",
      "Epoch [163/200]\n",
      "Avg loss: 59.958117 \n",
      "\n",
      "------------------------------\n",
      "Epoch [164/200]\n",
      "Avg loss: 60.070462 \n",
      "\n",
      "------------------------------\n",
      "Epoch [165/200]\n",
      "Avg loss: 59.696683 \n",
      "\n",
      "------------------------------\n",
      "Epoch [166/200]\n",
      "Avg loss: 59.736199 \n",
      "\n",
      "------------------------------\n",
      "Epoch [167/200]\n",
      "Avg loss: 59.707429 \n",
      "\n",
      "------------------------------\n",
      "Epoch [168/200]\n",
      "Avg loss: 59.790153 \n",
      "\n",
      "------------------------------\n",
      "Epoch [169/200]\n",
      "Avg loss: 60.039737 \n",
      "\n",
      "------------------------------\n",
      "Epoch [170/200]\n",
      "Avg loss: 59.921492 \n",
      "\n",
      "------------------------------\n",
      "Epoch [171/200]\n",
      "Avg loss: 59.574089 \n",
      "\n",
      "------------------------------\n",
      "Epoch [172/200]\n",
      "Avg loss: 59.364898 \n",
      "\n",
      "------------------------------\n",
      "Epoch [173/200]\n",
      "Avg loss: 59.468922 \n",
      "\n",
      "------------------------------\n",
      "Epoch [174/200]\n",
      "Avg loss: 59.728207 \n",
      "\n",
      "------------------------------\n",
      "Epoch [175/200]\n",
      "Avg loss: 59.527987 \n",
      "\n",
      "------------------------------\n",
      "Epoch [176/200]\n",
      "Avg loss: 59.448674 \n",
      "\n",
      "------------------------------\n",
      "Epoch [177/200]\n",
      "Avg loss: 59.752287 \n",
      "\n",
      "------------------------------\n",
      "Epoch [178/200]\n",
      "Avg loss: 59.650182 \n",
      "\n",
      "------------------------------\n",
      "Epoch [179/200]\n",
      "Avg loss: 59.727673 \n",
      "\n",
      "------------------------------\n",
      "Epoch [180/200]\n",
      "Avg loss: 59.297416 \n",
      "\n",
      "------------------------------\n",
      "Epoch [181/200]\n",
      "Avg loss: 58.980070 \n",
      "\n",
      "------------------------------\n",
      "Epoch [182/200]\n",
      "Avg loss: 58.910151 \n",
      "\n",
      "------------------------------\n",
      "Epoch [183/200]\n",
      "Avg loss: 59.171254 \n",
      "\n",
      "------------------------------\n",
      "Epoch [184/200]\n",
      "Avg loss: 59.065411 \n",
      "\n",
      "------------------------------\n",
      "Epoch [185/200]\n",
      "Avg loss: 59.319297 \n",
      "\n",
      "------------------------------\n",
      "Epoch [186/200]\n",
      "Avg loss: 58.912299 \n",
      "\n",
      "------------------------------\n",
      "Epoch [187/200]\n",
      "Avg loss: 58.701665 \n",
      "\n",
      "------------------------------\n",
      "Epoch [188/200]\n",
      "Avg loss: 58.668654 \n",
      "\n",
      "------------------------------\n",
      "Epoch [189/200]\n",
      "Avg loss: 58.907285 \n",
      "\n",
      "------------------------------\n",
      "Epoch [190/200]\n",
      "Avg loss: 59.307570 \n",
      "\n",
      "------------------------------\n",
      "Epoch [191/200]\n",
      "Avg loss: 59.134057 \n",
      "\n",
      "------------------------------\n",
      "Epoch [192/200]\n",
      "Avg loss: 59.189016 \n",
      "\n",
      "------------------------------\n",
      "Epoch [193/200]\n",
      "Avg loss: 59.206268 \n",
      "\n",
      "------------------------------\n",
      "Epoch [194/200]\n",
      "Avg loss: 59.031989 \n",
      "\n",
      "------------------------------\n",
      "Epoch [195/200]\n",
      "Avg loss: 59.191989 \n",
      "\n",
      "------------------------------\n",
      "Epoch [196/200]\n",
      "Avg loss: 59.417591 \n",
      "\n",
      "------------------------------\n",
      "Epoch [197/200]\n",
      "Avg loss: 59.046599 \n",
      "\n",
      "------------------------------\n",
      "Epoch [198/200]\n",
      "Avg loss: 59.107229 \n",
      "\n",
      "------------------------------\n",
      "Epoch [199/200]\n",
      "Avg loss: 59.007174 \n",
      "\n",
      "------------------------------\n",
      "Epoch [200/200]\n",
      "Avg loss: 59.252943 \n",
      "\n",
      "------------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# NeuralNetwork Class (Only Dense Layer)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(n_input, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(n_hidden, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "n_input = x_train.shape[1]   \n",
    "n_hidden = 64                \n",
    "\n",
    "# Functions\n",
    "model = NeuralNetwork(n_input, n_hidden).to(device)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train Function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train() \n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Test Function\n",
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:.6f} \\n\")\n",
    "\n",
    "# Train + Test Loop\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch [{t+1}/{epochs}]\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
